{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c65ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from math import inf\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import Tensor\n",
    "from torch.nn import Conv2d, Module, MSELoss, PixelShuffle, Sequential, Tanh, init\n",
    "from torch.optim import Adam\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.types import Device\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ff92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(in_channels, out_channels, kernel_size) -> Module:\n",
    "    return Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "\n",
    "def init_parameters(module):\n",
    "    if isinstance(module, Conv2d):\n",
    "        init.xavier_normal_(module.weight)\n",
    "        init.constant_(module.bias, 0.0)\n",
    "\n",
    "\n",
    "class ESPCN(Module):\n",
    "    SCALE = 3\n",
    "\n",
    "    N0 = 3\n",
    "    N1 = 64\n",
    "    N2 = 32\n",
    "    N3 = N0 * SCALE ** 2\n",
    "\n",
    "    F1 = 5\n",
    "    F2 = 3\n",
    "    F3 = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv2d(\n",
    "            in_channels=self.N0, out_channels=self.N1, kernel_size=self.F1\n",
    "        )\n",
    "        self.conv2 = conv2d(\n",
    "            in_channels=self.N1, out_channels=self.N2, kernel_size=self.F2\n",
    "        )\n",
    "        self.conv3 = conv2d(\n",
    "            in_channels=self.N2, out_channels=self.N3, kernel_size=self.F3\n",
    "        )\n",
    "\n",
    "        self.head = Sequential(self.conv1, Tanh(), self.conv2, Tanh())\n",
    "        self.tail = Sequential(self.conv3, PixelShuffle(self.SCALE))\n",
    "\n",
    "        self.sequential = Sequential(self.head, self.tail)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813d52f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class TensorPairsDataset(Dataset[Tuple[Tensor, Tensor]]):\n",
    "    def __init__(self, name):\n",
    "        h5 = h5py.File(name, \"r\", libver=\"latest\", swmr=True)\n",
    "\n",
    "        self.lr = h5[\"lr\"]\n",
    "        self.hr = h5[\"hr\"]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return torch.from_numpy(self.lr[index]), torch.from_numpy(self.hr[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(\n",
    "    model: Module,\n",
    "    criterion: Module,\n",
    "    device: Device,\n",
    "    optimizer: Optimizer,\n",
    "    loader: DataLoader,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Runs a training epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    epoch_mean_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(\n",
    "        tqdm(loader, unit=\"batch\", leave=False, desc=\"Training\"), start=1\n",
    "    ):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        loss = criterion(model(x), y)\n",
    "        epoch_mean_loss += (loss.item() - epoch_mean_loss) / batch\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_mean_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(\n",
    "    model: Module,\n",
    "    criterion: Module,\n",
    "    device: Device,\n",
    "    loader: DataLoader,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Validates the model.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    epoch_mean_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(\n",
    "        tqdm(loader, unit=\"batch\", leave=False, desc=\"Validation\"), start=1\n",
    "    ):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        loss = criterion(model(x), y)\n",
    "        epoch_mean_loss += (loss.item() - epoch_mean_loss) / batch\n",
    "\n",
    "    return epoch_mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeada72",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "EPOCHS = 300\n",
    "PATIENCE = 10\n",
    "\n",
    "checkpoints = Path(\"/content/drive/MyDrive/Checkpoints/\") / \"ESPCN (L2, Scheduled)\"\n",
    "checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "resume = None\n",
    "resume = checkpoints / \"0.pth\"\n",
    "\n",
    "# dataset = TensorPairsDataset(\"var/rphln-safebooru2020-medium.train.h5\")\n",
    "dataset = TensorPairsDataset(\n",
    "    \"/content/drive/MyDrive/Datasets/rphln-safebooru2020-medium.train.h5\"\n",
    ")\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "summary(ESPCN().to(device), input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(resume) if resume else {}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=False).split(dataset)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(folds):\n",
    "    if checkpoint and fold < checkpoint[\"fold\"]:\n",
    "        continue\n",
    "\n",
    "    checkpoints_ = checkpoints / str(fold)\n",
    "    checkpoints_.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=str(checkpoints_))\n",
    "\n",
    "    model = ESPCN().to(device)\n",
    "    model.apply(init_parameters)\n",
    "\n",
    "    parameters = [\n",
    "        {\"params\": model.head.parameters()},\n",
    "        {\"params\": model.tail.parameters(), \"lr\": LEARNING_RATE * 0.1},\n",
    "    ]\n",
    "\n",
    "    optimizer = Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "    if checkpoint:\n",
    "        start = checkpoint[\"epoch\"]\n",
    "        best = checkpoint[\"best\"]\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        checkpoint = {}\n",
    "    else:\n",
    "        start = 0\n",
    "        best = inf\n",
    "\n",
    "    training_loader = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        sampler=SubsetRandomSampler(training_indices),\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "        sampler=SubsetRandomSampler(validation_indices),\n",
    "    )\n",
    "\n",
    "    # How many epochs since the last improvement?\n",
    "    stale = 0\n",
    "\n",
    "    for epoch in tqdm(count(start=1 + start), unit=\"epoch\", desc=f\"Fold #{fold}\"):\n",
    "        epoch_training_loss = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "            optimizer,\n",
    "            training_loader,\n",
    "        )\n",
    "        epoch_validation_loss = validate(\n",
    "            model,\n",
    "            criterion,\n",
    "            device,\n",
    "            validation_loader,\n",
    "        )\n",
    "\n",
    "        if epoch_validation_loss < best:\n",
    "            best = epoch_validation_loss\n",
    "            stale = 0\n",
    "\n",
    "            stats = {\n",
    "                \"Training\": epoch_training_loss,\n",
    "                \"Validation\": epoch_validation_loss,\n",
    "            }\n",
    "            writer.add_scalars(\"Loss\", stats, epoch)\n",
    "\n",
    "            state = {\n",
    "                \"name\": model.__class__.__name__,\n",
    "                \"epoch\": epoch,\n",
    "                \"fold\": fold,\n",
    "                \"best\": best,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            torch.save(state, checkpoints_.with_suffix(\".pth\"))\n",
    "        else:\n",
    "            stale += 1\n",
    "\n",
    "        if stale >= PATIENCE:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}