{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b816c24e",
   "metadata": {},
   "source": [
    "# SRCNN\n",
    "\n",
    "O modelo descrito é o \"Super-Resolution Convolutional Neural Network\", ou SRCNN, criado por Chao Dong *et al.* em 2015.\n",
    "\n",
    "A rede é composta por três camadas convolucionais, intercaladas pela função de ativação ReLU. As imagens passam por uma interpolação bicúbica antes de serem alimentadas à rede.\n",
    "\n",
    "Aqui, usamos a configuração 9-5-5 para tamanho de filtros. A primeira camada produz 64 canais, enquanto a segunda camada produz 32 canais.\n",
    "\n",
    "```\n",
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "├─Conv2d: 1-1                            [-1, 64, 96, 96]          15,616\n",
    "├─Conv2d: 1-2                            [-1, 32, 96, 96]          51,232\n",
    "├─Conv2d: 1-3                            [-1, 3, 96, 96]           2,403\n",
    "==========================================================================================\n",
    "Total params: 69,251\n",
    "Trainable params: 69,251\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 637.30\n",
    "==========================================================================================\n",
    "Input size (MB): 0.01\n",
    "Forward/backward pass size (MB): 6.96\n",
    "Params size (MB): 0.26\n",
    "Estimated Total Size (MB): 7.24\n",
    "==========================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import Conv2d, Module, init\n",
    "from torch.nn.functional import interpolate, mse_loss, relu\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "\n",
    "def conv2d(in_channels, out_channels, kernel_size) -> Module:\n",
    "    return Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "\n",
    "def init_parameters(module):\n",
    "    if isinstance(module, Conv2d):\n",
    "        init.normal_(module.weight, std=1e-3)\n",
    "        init.constant_(module.bias, val=0e-0)\n",
    "\n",
    "\n",
    "class SRCNN(Module):\n",
    "    N0 = 3\n",
    "    N1 = 64\n",
    "    N2 = 32\n",
    "\n",
    "    F1 = 9\n",
    "    F2 = 5\n",
    "    F3 = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv2d(\n",
    "            in_channels=self.N0, out_channels=self.N1, kernel_size=self.F1\n",
    "        )\n",
    "        self.conv2 = conv2d(\n",
    "            in_channels=self.N1, out_channels=self.N2, kernel_size=self.F2\n",
    "        )\n",
    "        self.conv3 = conv2d(\n",
    "            in_channels=self.N2, out_channels=self.N0, kernel_size=self.F3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = interpolate(x, scale_factor=3, mode=\"bicubic\", align_corners=False)\n",
    "        x = relu(self.conv1(x), inplace=True)\n",
    "        x = relu(self.conv2(x), inplace=True)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a14d75",
   "metadata": {},
   "source": [
    "## Dependências comuns\n",
    "\n",
    "Definimos aqui as dependências comuns a todos os modelos treinados e o conjunto de treinamento; também exibimos uma pequena parcela, selecionada aleatoriamente, do conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from srnn import training\n",
    "from srnn.dataset import TensorPairsDataset\n",
    "\n",
    "dataset = TensorPairsDataset(\"var/rphln-safebooru2020-medium.train.h5\")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "lr, hr = next(iter(loader))\n",
    "\n",
    "display(to_pil_image(make_grid(hr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee04b6",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "Por fim, realizamos o treinamento da rede. O artigo especifica a taxa de aprendizado das duas primeiras camadas como 10<sup>-4</sup> e da última camada como 10<sup>-5</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "model = SRCNN()\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "parameters = [\n",
    "    {\"params\": model.conv1.parameters()},\n",
    "    {\"params\": model.conv2.parameters()},\n",
    "    {\"params\": model.conv3.parameters(), \"lr\": LEARNING_RATE * 0.1},\n",
    "]\n",
    "\n",
    "training(\n",
    "    model=model,\n",
    "    parameters=parameters,\n",
    "    init_parameters=init_parameters,\n",
    "    criterion=mse_loss,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epochs=300,\n",
    "    batch_size=256,\n",
    "    save_interval=10,\n",
    "    device=\"cuda:0\",\n",
    "    dataset=dataset,\n",
    "    checkpoints=Path(\"var/checkpoints/\"),\n",
    "    # resume=Path(\"var/checkpoints/SRCNN-L2-90-4.pth\"),\n",
    "    basename=\"{model}-Unnormalized-L2-{epoch}-{fold}.pth\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
